## Multilingual Hate Speech Modeling from a Perspectivist Viewpoint: Incorporating Inter-Annotator Disagreement into Model Training and Evaluation

# Abstract
As the number of social media users rapidly increases, so does the amount of toxic content published on social networking sites. As a result, the Machine Learning (ML) community has begun to take an interest in the automatic detection of hate speech, which is typically approached as a classification task. While modern ML algorithms are known to provide nearly human-like results for a variety of downstream Natural Language Processing (NLP) tasks, the classification of hate speech is still an open challenge due to its problematic annotation process, which often leads to disagreement between annotators. To address this issue, perspectivist approaches depart from the gold standard by introducing methods for learning from disagreement as opposed to disregarding it. Such approaches recognize the inherent subjectivity and ambiguity of hate speech and believe that including multiple perspectives in the training pipeline of ML models helps them learn from conflicting annotations, ultimately leading to improved performance in real-world scenarios. This thesis follows such a perspectivist approach by fine-tuning multilingual language models on the hate speech detection task using diamond standard data, as well as using appropriate evaluation metrics which take disagreement into account to measure their performance. The main contributions of this research include the demonstration that these multilingual models outperform their monolingual counterparts and perform competitively with models trained on gold standard data, underscoring the effectiveness of multilingual and perspectivist approaches in the nuanced task of hate speech detection.

